<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Recent Scrapy Posts from Tech Monger</title>
  <id>:///feeds/scrapy/</id>
  <updated>2018-11-10T07:16:38.730746Z</updated>
  <link href=":///" />
  <link href=":///feeds/scrapy/" rel="self" />
  <generator>Werkzeug</generator>
  <entry xml:base=":///feeds/scrapy/">
    <title type="text">Scrapy Get Redirect URLs and Request URL</title>
    <id>https://techmonger.github.io/63/scrapy-redirect-urls</id>
    <updated>2018-11-10T07:16:38.730746Z</updated>
    <published>2018-11-09T13:46:24.517791Z</published>
    <link href="https://techmonger.github.io/63/scrapy-redirect-urls" />
    <author>
      <name>Tech Monger</name>
    </author>
    <content type="html">&lt;p class=&quot;lead&quot;&gt;If you ever wanted to figure out all the redirect urls that scrapy spider hopped on or what is the currently requested URL by the spider then you easily get that using following &lt;a href=&quot;#example&quot;&gt;example code&lt;/a&gt;.</content>
  </entry>
  <entry xml:base=":///feeds/scrapy/">
    <title type="text">Scrapy Random Fake User Agent Example</title>
    <id>https://techmonger.github.io/62/example-scrapy-fake-user-agent</id>
    <updated>2018-10-19T19:16:03.373783Z</updated>
    <published>2018-10-19T18:30:06.053868Z</published>
    <link href="https://techmonger.github.io/62/example-scrapy-fake-user-agent" />
    <author>
      <name>Tech Monger</name>
    </author>
    <content type="html">&lt;p class=&quot;lead&quot;&gt;If sites you are crawling with scrapy dont respond to your request then you should use randomly generated user agent in your request. Scrapy Fake User Agent is one of the open source and useful extension which will help you evade bot detection programs easily.</content>
  </entry>
</feed>
