<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Recent Scrapy Posts from Tech Monger</title>
  <id>:///feeds/scrapy/</id>
  <updated>2018-10-19T19:16:03.373783Z</updated>
  <link href=":///" />
  <link href=":///feeds/scrapy/" rel="self" />
  <generator>Werkzeug</generator>
  <entry xml:base=":///feeds/scrapy/">
    <title type="text">Scrapy Random Fake User Agent Example</title>
    <id>https://techmonger.github.io/62/example-scrapy-fake-user-agent</id>
    <updated>2018-10-19T19:16:03.373783Z</updated>
    <published>2018-10-19T18:30:06.053868Z</published>
    <link href="https://techmonger.github.io/62/example-scrapy-fake-user-agent" />
    <author>
      <name>Tech Monger</name>
    </author>
    <content type="html">&lt;p class=&quot;lead&quot;&gt;If sites you are crawling with scrapy dont respond to your request then you should use randomly generated user agent in your request. Scrapy Fake User Agent is one of the open source and useful extension which will help you evade bot detection programs easily.</content>
  </entry>
</feed>
